{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Welcome to the <code>readnext</code> documentation! \u2600\ufe0f</p> <p>The <code>readnext</code> package provides a hybrid recommender system for computer science papers. Its main objective is to suggest relevant research papers based on a given query document you might be currently exploring, streamlining your journey to discover more intriguing academic literature.</p> <p>Check out the source code of the readnext package on GitHub. The motivation, theoretical background and results of this project are presented in the thesis. The accompanying repository contains the LaTeX source code as well as the code to generate all figures and diagrams in the thesis.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Source Code: readnext</li> <li>Thesis: Bridging Citation Analysis and Language Models: A Hybrid Recommender System for Computer Science Papers</li> </ul>"},{"location":"#overview","title":"Overview","text":"<ul> <li> <p>Installation: Get started with the package in no time with a simple installation guide.</p> </li> <li> <p>Overview: Obtain a high-level perspective of the project with a concise schematic overview.</p> </li> <li> <p>Setup: Build the foundation of inference by downloading the required data and model files. Set environment variables, run user scripts to precompute data required for inference, and get tips for an efficient development workflow.</p> </li> <li> <p>Inference: Get instructions on how to employ the package for inference, enabling you to extract recommendations for any query document of your choice.</p> </li> <li> <p>Reproducibility: Learn how to reproduce the evaluation results presented in the thesis.</p> </li> <li> <p>Customization: Customize the package to your needs by adding or replacing tokenizers, language models or evaluation metrics.</p> </li> </ul>"},{"location":"customization/","title":"Customization","text":"<p>This chapter guides you through the process of adding custom components to the <code>readnext</code> package tailored to your needs. While certain parts of the codebase are not intended for modification to maintain lean code (e.g., the features of the Citation Recommender), others are designed for easy extension.</p> <p>We utilize the Strategy Pattern to ensure adherence to the open-closed principle: New components can be added without modifying any of the existing code.</p> <p>In this chapter, we present three hands-on examples where customizing the <code>readnext</code> package is particularly straightforward. We will:</p> <ul> <li>Add a new tokenizer for the Language Recommender.</li> <li>Add a new language model for the Language Recommender.</li> <li>Add a new evaluation metric.</li> </ul> <p>By implementing a predefined interface, the new components can serve as drop-in replacements for the existing ones throughout the codebase.</p>"},{"location":"customization/#adding-evaluation-metrics","title":"Adding Evaluation Metrics","text":"<p>All evaluation metrics for a list of recommendations inherit from the abstract <code>EvaluationMetric</code> class.</p> <p>Source Code</p> <p>Explore the source code of the <code>EvaluationMetric</code> class here.</p> <p>To comply with the interface of the parent class, all evaluation metrics must implement the <code>score()</code> and <code>from_df()</code> methods:</p> <ul> <li>The <code>score()</code> method accepts any generic   Sequence type   or polars Series of integers or strings as input and computes a scalar value   from it.</li> <li>The <code>from_df()</code> method takes the original documents dataframe with label   columns <code>arxiv_labels</code> and <code>integer_label</code> as input and applies the   <code>score()</code> method to one of these columns.</li> </ul>"},{"location":"customization/#example","title":"Example","text":"<p>As an illustrative example, we implement the new <code>FirstRelevant</code> evaluation metric that takes a list of 0/1 for irrelevant/relevant recommendations as input and returns the position of the first relevant recommendation.</p> <p>Thus, lower (positive) values are better: If the top recommendation is indeed relevant, the score is 1, if the second recommendation is relevant but the first is not, the score is 2, and so on. If none of the recommendations are relevant, the score is set to 0.</p> <p>The following code snippet shows the implementation of the <code>FirstRelevant</code> class:</p> <pre><code>from dataclasses import dataclass\nfrom readnext.evaluation.metrics import EvaluationMetric\nfrom readnext.utils.aliases import DocumentsFrame\n@dataclass\nclass FirstRelevant(EvaluationMetric):\n\"\"\"\n    Computes the position of the first relevant item in a list of integer\n    recommendations.\n    \"\"\"\n@classmethod\ndef score(cls, label_list: IntegerLabelList) -&gt; int:\nif not len(label_list):\nreturn 0\nfor k, label in enumerate(label_list, 1):\nif label == 1:\nreturn k\nreturn 0\n@classmethod\ndef from_df(cls, df: DocumentsFrame) -&gt; int:\nreturn cls.score(df[\"integer_label\"])\n</code></pre> <p>The <code>score()</code> method first checks if the provided sequence is empty (in which case it returns early) and then uses the native <code>enumerate()</code> function to check if any label is equal to one. If so, this label position is returned. If not, the score is set to 0.</p> <p>Info</p> <p>The <code>score()</code> method is one of the rare cases where we could have used the <code>else</code> keyword of a <code>for</code> loop: If the loop is not exited early, i.e. if no label is equal to 1, the <code>else</code> block is executed and returns 0.</p> <p>The <code>from_df()</code> method simply calls the <code>score()</code> method on the <code>integer_label</code> column of the documents dataframe.</p> <p>Our new <code>FirstRelevant</code> metric can now be used anywhere in the codebase in place of the existing <code>AveragePrecision</code> and <code>CountUniqueLabels</code> evaluation metrics.</p>"},{"location":"customization/#adding-tokenizers","title":"Adding Tokenizers","text":"<p>In contrast to the case for evaluation metrics, tokenizers can inherit from two different abstract base classes, depending on whether they tokenize text into string tokens (such as <code>spacy</code>) or into integer token ids (such as all tokenizers from the <code>transformers</code> library):</p> <ol> <li>If they tokenize text into string tokens, they inherit from the <code>Tokenizer</code>    class. To comply with the interface of the parent class, they must implement    the <code>tokenize_single_document()</code> method, which takes a single string as input    and returns a list of string tokens. Then, they inherit the <code>tokenize()</code>    method which is used to construct a polars dataframe with a <code>tokens</code> column    that contains the tokenized documents.</li> <li>If they tokenize text into integer token ids, they inherit from the    <code>TorchTokenizer</code> class. Similar to the case above, all child classes must    implement a <code>tokenize_into_ids()</code> method that either takes a single string    as input and returns a list of integer token ids or a list of strings as input    and returns a list of lists of integer token ids. Then, they inherit the    <code>tokenize()</code> method which is used to construct a polars dataframe with a    <code>token_ids</code> column that contains the tokenized documents.</li> </ol> <p>Source Code</p> <p>Take a look at the source code of the <code>Tokenizer</code> class here and the source code of the <code>TorchTokenizer</code> class here.</p>"},{"location":"customization/#example_1","title":"Example","text":"<p>As an example, we add the <code>NaiveTokenizer</code> that simply converts all characters to lowercase and splits the text at whitespace characters.</p> <p>Since the <code>NaiveTokenizer</code> tokenizes text into string tokens, it inherits from the <code>Tokenizer</code> class and must implement the <code>tokenize_single_document()</code> method:</p> <pre><code>from dataclasses import dataclass\nfrom readnext.utils.aliases import Tokens\nfrom readnext.modeling.language_models import Tokenizer\n@dataclass\nclass NaiveTokenizer(Tokenizer):\n\"\"\"\n    Naive tokenizer that splits text at whitespace characters and converts all\n    characters to lowercase.\n    \"\"\"\ndef tokenize_single_document(self, document: str) -&gt; Tokens:\nreturn document.lower().split()\n</code></pre> <p>The new <code>NaiveTokenizer</code> can now be used anywhere in the codebase in place of the existing <code>SpacyTokenizer</code>.</p>"},{"location":"customization/#adding-language-models","title":"Adding Language Models","text":"<p>Similar to Tokenizers, Language Models can inherit from two different abstract base classes, depending on whether they are a <code>transformers</code> model or not:</p> <ul> <li>If they are not a <code>transformers</code> model, they inherit from the <code>Embedder</code> class.   In this case, they must implement the <code>compute_embedding_single_document()</code>   and <code>compute_embeddings_frame()</code> methods. The former takes a <code>Tokens</code>   (a list of strings) as input and returns an <code>Embedding</code> (a list of floats).   The latter uses the <code>tokens_frame</code> from the instance initialization and returns   a polars DataFrame with two columns named <code>d3_document_id</code> and <code>embedding</code>.</li> <li>If they are a <code>transformers</code> model, they inherit from the <code>TorchEmbedder</code>   class. In this case, they do not have to implement any methods in the child   class since the interface for all <code>transformers</code> models is identical. For the   purpose of correct typing, any child class must only define the <code>torch_model</code>   attribute with the correct type annotation.</li> </ul> <p>Source Code</p> <p>Take a look at the source code of the <code>Embedder</code> class here and the source code of the <code>TorchEmbedder</code> class here.</p>"},{"location":"customization/#example_2","title":"Example","text":"<p>As an example, we add the <code>DummyEmbedder</code> that simply returns a list of zeros for each document. To simulate a 300-dimensional embedding, we set the length of the list to 300.</p> <p>Since the <code>DummyEmbedder</code> is not a <code>transformers</code> model, it inherits from the <code>Embedder</code> class and must implement the <code>compute_embedding_single_document()</code> and <code>compute_embeddings_frame()</code> methods:</p> <pre><code>from dataclasses import dataclass\nfrom readnext.utils.aliases import EmbeddingsFrame, Tokens\nfrom readnext.modeling.language_models import Embedder\n@dataclass\nclass DummyEmbedder(Embedder):\n\"\"\"\n    Dummy embedder that returns a list of zeros for each document.\n    \"\"\"\ndef compute_embedding_single_document(self, document_tokens: Tokens) -&gt; Embedding:\n# although the `document_tokens` argument is not used, it is necessary\n# to implement the interface properly\nreturn [0] * 300\ndef compute_embeddings_frame(self) -&gt; EmbeddingsFrame:\nreturn self.tokens_frame.with_columns(\nembedding=pl.Series([[0] * 300] * self.tokens_frame.height)\n).drop(\"tokens\")\n</code></pre> <p>The new <code>DummyEmbedder</code> can now be used anywhere in the codebase in place of any existing embedder that is not based on a <code>transformers</code> model. This includes the <code>TFIDFEmbedder</code>, <code>BM25Embedder</code>, <code>Word2VecEmbedder</code>, <code>GloveEmbedder</code>, and <code>FastTextEmbedder</code> classes.</p>"},{"location":"inference/","title":"Inference","text":"<p>The user interface for generating recommendations is designed to be simple and easy to use. It relies on the top-level <code>readnext()</code> function, which takes two required and one optional keyword argument:</p> <ul> <li>An identifier for the query paper. This can be the Semanticscholar ID, Semanticscholar URL, Arxiv ID, or Arxiv URL of the paper. This argument is required and should be provided as a string.</li> </ul> <p>Term Definitions</p> <ul> <li>The Semanticscholar ID is a 40-digit hexadecimal string at the end of the Semanticscholar URL after the last forward slash. For example, the Semanticscholar ID for the URL <code>https://www.semanticscholar.org/paper/67c4ffa7f9c25e9e0f0b0eac5619070f6a5d143d</code> is <code>67c4ffa7f9c25e9e0f0b0eac5619070f6a5d143d</code>.</li> <li>The Arxiv ID is a 4-digit number followed by a dot followed by a 5-digit number at the end of the Arxiv URL after the last forward slash. For example, the Arxiv ID for the URL <code>https://arxiv.org/abs/1234.56789</code> is <code>1234.56789</code>.</li> </ul> <ul> <li> <p>The language model choice for the Language Recommender, which is used to tokenize and embed the query paper's abstract. This argument is required and should be passed using the <code>LanguageModelChoice</code> Enum, which provides autocompletion for all eight available language models.</p> </li> <li> <p>The feature weighting for the Citation Recommender. This argument is optional and is submitted using an instance of the <code>FeatureWeights</code> class. If not specified, the five features (<code>publication_date</code>, <code>citationcount_document</code>, <code>citationcount_author</code>, <code>co_citation_analysis</code>, and <code>bibliographic_coupling</code>) are given equal weights of one. Note that the weights are normalized to sum up to one, so the absolute values are irrelevant; only the relative ratios matter.</p> </li> </ul>"},{"location":"inference/#examples","title":"Examples","text":"<p>Inference works for both 'seen' and 'unseen' query documents, depending on whether the query document is part of the training corpus or not.</p>"},{"location":"inference/#seen-query-paper","title":"Seen Query Paper","text":"<p>If the query paper is part of the training corpus, all feature values are precomputed and inference is fast.</p> <p>Assume that we have just read the groundbreaking paper \"Attention is all you need\" by Vaswani et al. (2017) and want to find related papers that could be of interest to us.</p> <p>The following code example illustrates the standard usage of the <code>readnext()</code> function to retrieve recommendations. In this case we use the ArXiV URL as identifier, the FastText language model and assign a custom weighting scheme to the Citation Recommender:</p> <pre><code>from readnext import readnext, LanguageModelChoice, FeatureWeights\nresult = readnext(\n# `Attention is all you need` query paper\narxiv_url=\"https://arxiv.org/abs/1706.03762\",\nlanguage_model_choice=LanguageModelChoice.FASTTEXT,\nfeature_weights=FeatureWeights(\npublication_date=1,\ncitationcount_document=2,\ncitationcount_author=0.5,\nco_citation_analysis=2,\nbibliographic_coupling=2,\n),\n)\n</code></pre> <p>A message is printed to the console indicating that the query paper is part of the training corpus:</p> <pre><code>&gt; \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n&gt; \u2502                                                  \u2502\n&gt; \u2502 Query document is contained in the training data \u2502\n&gt; \u2502                                                  \u2502\n&gt; \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The return value of the <code>readnext()</code> function contains the following attributes:</p> <ul> <li> <p><code>document_identifier</code>: Contains the identifiers of the query paper.</p> </li> <li> <p><code>document_info</code>: Provides information about the query paper.</p> </li> <li> <p><code>features</code>: Individual dataframes that include values for <code>publication_date</code>, <code>citationcount_document</code>, <code>citationcount_author</code>, <code>co_citation_analysis</code>, <code>bibliographic_coupling</code>, <code>cosine_similarity</code>, and <code>feature_weights</code>.</p> </li> <li> <p><code>ranks</code>: Individual dataframes that list the ranks of individual features.</p> </li> <li> <p><code>points</code>: Individual dataframes that specify the points of individual features.</p> </li> <li> <p><code>labels</code>: Individual dataframes that present the arxiv labels for all candidate papers and binary 0/1 labels related to the query paper. These binary labels are useful for 'seen' query papers where the arxiv labels of the query paper is known. For 'unseen' papers this information is not availabels and all binary labels are set to 0.</p> </li> <li> <p><code>recommendations</code>: Individual dataframes that offer the top paper recommendations. Recommendations are calculated for both Hybrid-Recommender orders (Citation -&gt; Language and Language -&gt; Citation) and both the intermediate candidate lists and the final hybrid recommendations.</p> </li> </ul> <pre><code>print(result.recommendations.language_to_citation.head(10))\n</code></pre> candidate_d3_document_id weighted_points title author arxiv_labels integer_label semanticscholar_url arxiv_url publication_date publication_date_points citationcount_document citationcount_document_points citationcount_author citationcount_author_points co_citation_analysis_score co_citation_analysis_points bibliographic_coupling_score bibliographic_coupling_points 11212020 80.3 Neural Machine Translation by Jointly Learning to Align and Translate Yoshua Bengio ['cs.CL' 'cs.LG' 'cs.NE' 'stat.ML'] 1 https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5 https://arxiv.org/abs/1409.0473 2014-09-01 0 19996 88 372099 100 45 93 4 95 7961699 70.9 Sequence to Sequence Learning with Neural Networks Ilya Sutskever ['cs.CL' 'cs.LG'] 1 https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d https://arxiv.org/abs/1409.3215 2014-09-10 0 15342 83 234717 0 25 86 5 97 1629541 58.9 Fully convolutional networks for semantic segmentation Trevor Darrell ['cs.CV'] 0 https://www.semanticscholar.org/paper/317aee7fc081f2b137a85c4f20129007fd8e717e https://arxiv.org/abs/1411.4038 2014-11-14 0 25471 91 142117 0 20 81 0 49 206593880 57.9 Rethinking the Inception Architecture for Computer Vision Christian Szegedy ['cs.CV'] 0 https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58 https://arxiv.org/abs/1512.00567 2015-12-02 0 16562 85 128072 0 21 83 0 49 10716717 56.8 Feature Pyramid Networks for Object Detection Kaiming He ['cs.CV'] 0 https://www.semanticscholar.org/paper/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36 https://arxiv.org/abs/1612.03144 2016-12-09 0 10198 70 251467 0 14 71 1 72 6287870 55.7 TensorFlow: A system for large-scale machine learning J. Dean ['cs.DC' 'cs.AI'] 0 https://www.semanticscholar.org/paper/46200b99c40e8586c8a0f588488ab6414119fb28 https://arxiv.org/abs/1605.08695 2016-05-27 0 13266 77 115104 0 4 33 7 99 3429309 52.8 DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs A. Yuille ['cs.CV'] 0 https://www.semanticscholar.org/paper/cab372bc3824780cce20d9dd1c22d4df39ed081a https://arxiv.org/abs/1606.00915 2016-06-02 0 9963 69 64894 0 9 57 1 72 4555207 52.8 MobileNetV2: Inverted Residuals and Linear Bottlenecks Liang-Chieh Chen ['cs.CV'] 0 https://www.semanticscholar.org/paper/dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4 https://arxiv.org/abs/1801.04381 2018-01-13 0 7925 56 39316 0 10 60 2 82 13740328 52.5 Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification Kaiming He ['cs.CV' 'cs.AI' 'cs.LG'] 1 https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23 https://arxiv.org/abs/1502.01852 2015-02-06 0 12933 76 251467 0 6 49 1 72 225039882 52.3 An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Jakob Uszkoreit ['cs.CV' 'cs.AI' 'cs.LG'] 1 https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a https://arxiv.org/abs/2010.11929 2020-10-22 0 5519 16 51813 0 185 98 2 82 <p>Thus, the top recommendation is the paper \"Neural Machine Translation by Jointly Learning to Align and Translate\" by Yoshua Bengio.</p> <p>Recall that the second Recommender of the hybrid structure is responsible for re-ranking the candidate list. Thus, the selection of the recommendations above is performed by the Language Recommender based on cosine similarity whereas the order of the final recommendations is determined by the Citation Recommender based on the weighted points score in the second column.</p> <p>Tip</p> <p>In situations where the co-citation analysis and bibliographic coupling scores do not differ much between the top candidate papers, the author citation count tends to influence the result heavily! To obtain balanced results, it is recommended to downscale the <code>citationcount_author</code> weight compared to some of the other features.</p>"},{"location":"inference/#continue-the-flow","title":"Continue the Flow","text":"<p>We have read the previously recommended paper, but want to dive even deeper into the literature. Note that the Semanticscholar URL and the ArXiV URL of the recommendations are contained in the output table. Thus, they can be immediately used as identifiers for a new query!</p> <p>In this case, we use the <code>SciBERT</code> language model and the default feature weights of 1 for each feature to demonstrate the minimal set of inputs of the <code>readnext()</code> function:</p> <pre><code>from readnext import readnext, LanguageModelChoice, FeatureWeights\nresult = readnext(\n# `Attention is all you need` query paper\narxiv_url=\"https://arxiv.org/abs/1706.03762\",\nlanguage_model_choice=LanguageModelChoice.FASTTEXT,\nfeature_weights=FeatureWeights(\npublication_date=1,\ncitationcount_document=2,\ncitationcount_author=0.5,\nco_citation_analysis=2,\nbibliographic_coupling=2,\n),\n)\n# extract one of the paper identifiers from the previous top recommendation\nsemanticscholar_url = result.recommendations.citation_to_language[0, \"semanticscholar_url\"]\nresult_seen_query = readnext(\nsemanticscholar_url=semanticscholar_url,\nlanguage_model_choice=LanguageModelChoice.SCIBERT,\n)\n</code></pre> <p>Let's first take a look at our new query paper:</p> <pre><code>print(result_seen_query.document_info)\n</code></pre> <pre><code>&gt; Document 11212020\n&gt; ---------------------\n&gt; Title: Neural Machine Translation by Jointly Learning to Align and Translate\n&gt; Author: Yoshua Bengio\n&gt; Publication Date: 2014-09-01\n&gt; Arxiv Labels: ['cs.CL', 'cs.LG', 'cs.NE', 'stat.ML']\n&gt; Semanticscholar URL: https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\n&gt; Arxiv URL: https://arxiv.org/abs/1409.0473\n</code></pre> <p>In contrast to the previous example, we now choose the Citation -&gt; Language Hybrid-Recommender order.</p> <p>In this case, the rows are sorted in descending order by the cosine similarity between the query paper and the candidate papers since the re-ranking step is performed by the Language Recommender.</p> <p>For brevity we limit the output to the top three recommendations:</p> <pre><code>print(result_seen_query.recommendations.citation_to_language.head(3))\n</code></pre> candidate_d3_document_id cosine_similarity title author publication_date arxiv_labels integer_label semanticscholar_url arxiv_url 7961699 0.959 Sequence to Sequence Learning with Neural Networks Ilya Sutskever 2014-09-10 ['cs.CL' 'cs.LG'] 1 https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d https://arxiv.org/abs/1409.3215 5590763 0.9537 Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation Yoshua Bengio 2014-06-03 ['cs.CL' 'cs.LG' 'cs.NE' 'stat.ML'] 1 https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e https://arxiv.org/abs/1406.1078 1998416 0.9467 Effective Approaches to Attention-based Neural Machine Translation Christopher D. Manning 2015-08-17 ['cs.CL'] 1 https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0 https://arxiv.org/abs/1508.04025 <p>Hence, we might read the paper \"Sequence to Sequence Learning with Neural Networks\" by Ilya Sutskever et al. next.</p> <p>Tip</p> <p>If you are interested in the additional Citation Recommender feature values that were used to generate the candidate list, you can access them via the <code>recommendations.citation_to_language_candidates</code> attribute of the <code>result_seen_query</code> object.</p>"},{"location":"inference/#unseen-query-paper","title":"Unseen Query Paper","text":"<p>If the query paper is not part of the training corpus, the inference step takes longer since tokenization, embedding and the computation of co-citation analysis, bibliographic coupling and cosine similarity scores has to be performed from scratch.</p> <p>However, apart from a longer waiting time, the user does not have to care about if the query paper is part of the training corpus or not since the user interface remains the same!</p> <p>As an example, we fetch recommendations for the \"GPT-4 Technical Report\" paper by OpenAI. This paper is too recent to be part of the training corpus.</p> <p>Due to its recency, it might not have been cited that often, so we lower the weight of the <code>co_citation_analysis</code> feature. Further, we increase the <code>publication_date</code> weight and decrease the <code>citationcount_author</code> weight. For the Language Recommender we use the <code>GloVe</code> model to embed the paper abstract.</p> <p>Note that we only need to specify the weights for the features we want to change from the default value of 1</p> <pre><code>from readnext import readnext, LanguageModelChoice, FeatureWeights\nresult_unseen_query = readnext(\narxiv_url=\"https://arxiv.org/abs/2303.08774\",\nlanguage_model_choice=LanguageModelChoice.GLOVE,\nfeature_weights=FeatureWeights(\npublication_date=4,\ncitationcount_author=0.2,\nco_citation_analysis=0.2,\n),\n)\n</code></pre> <p>The console output informs us that the query paper is not part of the training corpus and provides some progress updates for the ongoing computations:</p> <pre><code>&gt; \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n&gt; \u2502                                                      \u2502\n&gt; \u2502 Query document is not contained in the training data \u2502\n&gt; \u2502                                                      \u2502\n&gt; \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n&gt; Loading training corpus................. \u2705 (0.07 seconds)\n&gt; Tokenizing query abstract............... \u2705 (0.41 seconds)\n&gt; Loading pretrained Glove model.......... \u2705 (26.38 seconds)\n&gt; Embedding query abstract................ \u2705 (0.00 seconds)\n&gt; Loading pretrained embeddings........... \u2705 (0.19 seconds)\n&gt; Computing cosine similarities........... \u2705 (0.05 seconds)\n</code></pre> <p>The time distribution differs between the language models. For <code>GloVe</code>, loading the large pretrained model into memory allocates by far the most time.</p> <p>Now, we generate the recommendations candidate list with the Language Recommender and re-rank the candidates with the Citation Recommender. Since the second recommender of the hybrid structure is the Citation Recommender, the output is again sorted by the weighted points score of the individual features:</p> <pre><code>print(result_unseen_query.recommendations.language_to_citation.head(3))\n</code></pre> candidate_d3_document_id weighted_points title author arxiv_labels integer_label semanticscholar_url arxiv_url publication_date publication_date_points citationcount_document citationcount_document_points citationcount_author citationcount_author_points co_citation_analysis_score co_citation_analysis_points bibliographic_coupling_score bibliographic_coupling_points 247951931 80 PaLM: Scaling Language Modeling with Pathways Noam M. Shazeer ['cs.CL'] 0 https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb https://arxiv.org/abs/2204.02311 2022-04-05 99 145 0 51316 0 72 99 77 96 230435736 14.6 The Pile: An 800GB Dataset of Diverse Text for Language Modeling Jason Phang ['cs.CL'] 0 https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e https://arxiv.org/abs/2101.00027 2020-12-31 19 154 0 1303 0 17 86 48 0 227239228 13.3 Pre-Trained Image Processing Transformer W. Gao ['cs.CV' 'cs.LG'] 0 https://www.semanticscholar.org/paper/43cb4886a8056d5005702edbc51be327542b2124 https://arxiv.org/abs/2012.00364 2020-12-01 5 379 0 13361 0 1 0 52 65 <p>The top recommendation introducing the PaLM language model as a competitor to the GPT family seems quite reasonable.</p> <p>Note that the <code>integer_label</code> column is not informative for unseen query papers and only kept for consistency. Since no arxiv labels are available for unseen query papers they can not intersect with the arxiv labels of the candidates such that all values of the <code>integer_label</code> column are set to 0.</p> <p>Tip</p> <p>If you are interested in the cosine similarity values that were used to generate the candidate list, you can access them via the <code>recommendations.language_to_citation_candidates</code> attribute of the <code>result_unseen_query</code> object.</p>"},{"location":"inference/#input-validation","title":"Input Validation","text":"<p>The <code>pydantic</code> library is used for basic input validation. For invalid user inputs the command fails early before any computations are performed with an informative error message.</p> <p>The following checks are performed:</p> <ul> <li> <p>The Semanticscholar ID must be a 40-character hexadecimal string.</p> </li> <li> <p>The Semanticscholar URL must be a valid URL starting with <code>https://www.semanticscholar.org/paper/</code>.</p> </li> <li> <p>The Arxiv ID must start with 4 digits followed by a dot followed by 5 more digits (e.g. <code>1234.56789</code>).</p> </li> <li> <p>The Arxiv URL must be a valid URL starting with <code>https://arxiv.org/abs/</code>.</p> </li> <li> <p>At least one of the four query paper identifiers must be provided.</p> </li> <li> <p>The feature weights must be non-negative numeric values.</p> </li> </ul> <p>For example, the following command fails because we assigned a negative weight to the <code>publication_date</code> feature:</p> <pre><code>from readnext import readnext, LanguageModelChoice, FeatureWeights\nresult = readnext(\narxiv_id=\"2101.03041\",\nlanguage_model_choice=LanguageModelChoice.BM25,\nfeature_weights=FeatureWeights(publication_date=-1),\n)\n</code></pre> <pre><code>pydantic.error_wrappers.ValidationError: 1 validation error for FeatureWeights\npublication_date\n  ensure this value is greater than or equal to 0 (type=value_error.number.not_ge; limit_value=0)\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Currently, the <code>readnext</code> package is not available on PyPI but can be installed directly from GitHub.</p> <p>This project requires Python 3.10. Earlier versions of Python are not supported. Future support for higher versions will be available once the <code>torch</code> and <code>transformers</code> libraries are fully compatible with Python 3.11 and beyond.</p> HTTPSSSH <pre><code>pip install git+https://github.com/joel-beck/readnext.git#egg=readnext\n</code></pre> <pre><code>pip install git+ssh://git@github.com/joel-beck/readnext.git#egg=readnext\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>Note: This section provides only a surface-level overview of the project. For a more detailed presentation, see chapter 3 of the thesis.</p>"},{"location":"overview/#hybrid-recommender","title":"Hybrid Recommender","text":"<p>The Hybrid Recommender involves a Citation Recommender that combines global document characteristics and citation-based features of papers, and a Language Recommender that employs a language model to generate embeddings from paper abstracts.</p> <p>The hybrid recommender combines the Citation Recommender and the Language Recommender in a cascade fashion, i.e. one is used to generate a candidate list which is then re-ranked by the other recommender.</p> <p>The candidate lists and final rankings of both hybrid orderings are evaluated using the Mean Average Precision (MAP) metric. The objectives of the evaluation are:</p> <ol> <li>Identify the best feature weights for the Citation Recommender.</li> <li>Identify the best language model for the Language Recommender.</li> <li>Assess whether the hybridization ordering, i.e. if the Citation or the Language Recommender is applied first, influences the Hybrid Recommender's performance.</li> </ol>"},{"location":"overview/#citation-recommender","title":"Citation Recommender","text":"<p>The Citation Recommender uses three global document features and two citation-based features:</p> <ol> <li> <p>Global Document Features</p> <p>These features are derived from the document metadata:</p> <ul> <li> <p>Publication Date:     A novelty metric. Recent publications score higher, as they build upon earlier papers and compare their findings with existing results.</p> </li> <li> <p>Paper Citation Count:     A document popularity metric. Papers with more citations are, on average and without any prior knowledge, considered more valuable and relevant.</p> </li> <li> <p>Author Citation Count:     An author popularity metric. Authors with higher total citations across their publications are deemed more important in the research community.</p> </li> </ul> </li> <li> <p>Citation-Based Features</p> <ul> <li> <p>Co-Citation Analysis:     Counts the number of shared citations, which in this context is equivalent to shared citing papers. These are papers that themselves cite both the query and the candidate paper. Candidate documents with higher co-citation analysis scores are considered more relevant to the query document.</p> </li> <li> <p>Bibliographic Coupling:     Counts the number of shared references or shared cited papers, i.e. papers that appear in the bibliography of both the query and the candidate paper. Candidate documents with higher bibliographic coupling scores are considered more relevant to the query document.</p> </li> </ul> <p>    Left: Bibliographic coupling counts the number of shared references. Paper A and Paper B are connected by bibliographic coupling since they both cite the same Paper C. Right: Co-citation analysis counts the number of shared citing papers. Here, Paper A and Paper B are connected by co-citation analysis since they are both cited by Paper C.    </p> </li> </ol> <p>Feature Weighting</p> <p>The five features of the Citation Recommender are combined linearly with user-specified feature weights. The weights are normalized with the L1 norm, ensuring the results are not affected by the absolute magnitude of the weights. A caveat of this approach is that the raw feature values, such as the publication date (represented as a date) and the paper citation count (an integer), are not directly comparable. To aggregate all five features into a single score, a rank-based method is used.</p> <p>The Citation Recommender first ranks all candidate papers according to each of the five features individually. The ranking process assigns the top rank 1 to the most relevant candidate paper and increments the rank by 1 for each subsequent paper. Candidate papers with more recent publication dates, higher citation counts, higher co-citation analysis and higher bibliographic coupling scores receive better rankings.</p> <p>Finally, those candidate papers with the lowest weighted rank are recommended to the user.</p> <p>Note: The true weighting scheme involves some additional steps that add interpretability but are conceptually equivalent to the version described above. See chapter 3.3 of the thesis for more details.</p>"},{"location":"overview/#language-recommender","title":"Language Recommender","text":"<p>The Language Recommender encodes paper abstracts into embedding vectors to capture semantic meaning. Candidate papers with embeddings most similar to the query embedding (measured by cosine similarity) are recommended.</p> <p>8 language models across 3 categories are implemented: keyword-based sparse embedding models, static embedding models, and contextual embedding models.</p> <p>Keyword-based models</p> <p>They produce sparse vector embeddings where the embedding dimension equals the vocabulary size of all document abstracts in the training corpus. For these models, text preprocessing and tokenization is performed by the <code>spaCy</code> library using the <code>en_core_web_sm</code> model.</p> <p>The following keyword-based models are considered:</p> <ul> <li> <p>TF-IDF: Implemented with <code>scikit-learn</code> according to the formula:</p> \\[\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\text{IDF}(t)\\] <p>with:</p> \\[\\text{TF}(t, d) = \\text{count}(t, d)\\] <p>and:</p> \\[\\text{IDF}(t) = \\log\\left(\\frac{1 + N}{1 + \\text{DF}(t)} + 1\\right)\\] <p>where:</p> <ul> <li>\\(t\\) is a token,</li> <li>\\(d\\) is a document,</li> <li>\\(\\text{TF}(t, d)\\) is the term frequency of token \\(t\\) in document \\(d\\) (interpreted as the relative frequency of a term in a document),</li> <li>\\(\\text{IDF}(t)\\) is the inverse document frequency of token \\(t\\) across all documents in the training corpus,</li> <li>\\(\\text{count}(t, d)\\) is the count of token \\(t\\) in document \\(d\\),</li> <li>\\(\\text{DF}(t)\\) is the document frequency of token \\(t\\) (the number of documents in the corpus that contain the term \\(t\\)),</li> <li>\\(N\\) is the total number of documents in the corpus.</li> </ul> <p>Finally, the TF-IDF vectors are normalized to unit length by the Euclidean norm.</p> </li> <li> <p>BM25: Implemented in the BM25+ variant as proposed by (Lv &amp; Zhai, 2011) and described in (Trotman et al., 2014).</p> <p>The formula is:</p> \\[\\text{BM25}(t, d) = \\text{BM25-TF}(t, d) \\cdot \\text{BM25-IDF}(t)\\] <p>with:</p> \\[\\text{BM25-TF}(t, d) = \\frac{(k + 1) \\cdot \\text{TF}(t, d)}{k \\cdot (1 - b + b \\cdot (\\text{len}(d) / \\text{avgdl})) + \\text{TF}(t, d)} + \\delta\\] <p>and:</p> \\[\\text{BM25-IDF}(t) = \\log\\left(\\frac{N+1}{\\text{DF}(t)}\\right)\\] <p>where:</p> <ul> <li>\\(t\\) is a token,</li> <li>\\(d\\) is a document,</li> <li>\\(\\text{BM25-TF}(t, d)\\) is the BM25+ term frequency of token \\(t\\) in document \\(d\\),</li> <li>\\(\\text{BM25-IDF}(t)\\) is the BM25+ inverse document frequency of token \\(t\\) across all documents in the training corpus,</li> <li>\\(\\text{TF}(t, d)\\) is the term frequency of token \\(t\\) in document \\(d\\) (interpreted as the relative frequency of a term in a document),</li> <li>\\(\\text{DF}(t)\\) is the document frequency of token \\(t\\) (the number of documents in the corpus that contain the term \\(t\\)),</li> <li>\\(\\text{len}(d)\\) is the total number of tokens in document \\(d\\),</li> <li>\\(\\text{avgdl}\\) is the average document length across the corpus,</li> <li>\\(N\\) is the total number of documents in the corpus,</li> <li>\\(k\\), \\(b\\), and \\(\\delta\\) are free parameters.</li> </ul> <p>Default values of \\(k = 1.5\\), \\(b = 0.75\\), and \\(\\delta = 1.0\\) are adapted from the rank_bm25 package.</p> </li> </ul> <p>Static embedding models</p> <p>They produce dense vector embeddings where the embedding dimension is fixed (here set to the default of 300) and independent of the vocabulary size. Word embeddings are averaged dimension-wise to obtain a single embedding vector for each abstract. Again, <code>spaCy</code> is used for text preprocessing and tokenization. All three static embedding models are pretrained and implemented via their <code>gensim</code> interface:</p> <ul> <li>Word2Vec: Pretrained on the Google News corpus using the <code>word2vec-google-news-300</code> gensim model.</li> <li>GloVe: Pretrained on the Gigaword corpus and Wikipedia using the <code>glove.6B.300d</code> model from the NLP Stanford GloVe project.</li> <li>FastText: Pretrained on the Common Crawl corpus and Wikipedia using the <code>cc.en.300.bin</code> model from the FastText Website.</li> </ul> <p>Contextual embedding models</p> <p>Similar to static embedding models, they produce dense vector embeddings where the embedding dimension is fixed (here set to the default of 768) and independent of the vocabulary size. Instead of string tokens, contextual embedding models take integer token IDs as input which are mapped to words and subwords and learned during pretraining. All three static embedding models are pretrained and implemented via the HuggingFace <code>transformers</code> library:</p> <ul> <li>BERT: Pretrained on the BooksCorpus and English Wikipedia using the <code>bert-base-uncased</code> model.</li> <li>SciBERT: Pretrained on the Semantic Scholar corpus (i.e. specific to scientific language) using the <code>allenai/scibert_scivocab_uncased</code> model.</li> <li>Longformer: Pretrained on the BooksCorpus and English Wikipedia using the <code>allenai/longformer-base-4096</code> model.</li> </ul> <p>Instead of averaging word embeddings like static embedding models, these Transformer based models cut off the document abstracts at a maximum token length of 512 for BERT and SciBERT and 4096 for the Longformer model. However, only 0.58% of all abstracts in the training corpus exceed the maximum token length of 512 such that the impact of this cutoff is negligible.</p>"},{"location":"overview/#labels","title":"Labels","text":"<p>To determine whether the Hybrid Recommender generates relevant or irrelevant recommendations, arXiV categories are used as labels. Within the Computer Science domain there are 40 different arXiV categories, such as <code>cs.AI</code> for Artificial Intelligence or <code>cs.CL</code> for Computation and Language. Importantly, each paper is not constrained to a single category but can be assigned to multiple categories.</p> <p>Based on these labels, a binary classification task is defined: A candidate paper is considered a relevant recommendation if it shares at least one arXiV label with the query paper, and irrelevant otherwise. For instance, if the query paper is assigned to the <code>cs.CL</code> and <code>cs.IR</code> categories, the candidate paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Devlin et al. (2018) is considered a relevant recommendation because it is assigned to the <code>cs.CL</code> category. Hence, there is an overlap between the query and candidate paper's arXiV labels. In contrast, the candidate paper Deep Residual Learning for Image Recognition by He et al. (2016) is considered an irrelevant recommendation because it is only assigned to the <code>cs.CV</code> category, which does not overlap with any of the query paper's categories.</p>"},{"location":"overview/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>The Mean Average Precision (MAP) is used as the primary evaluation metric to assess the performance of the Hybrid Recommender.</p> <p>Although many evaluation metrics are available for recommender systems, the MAP is chosen due to the following reasons:</p> <ol> <li>It takes the order of recommendations into account, i.e. it is not only important to recommend relevant items but also to recommend them early in the list.</li> <li>All items on the recommendation list are considered, i.e. it is not only important to recommend relevant items but also to avoid irrelevant items.</li> <li>It works well with binary 0/1 encoded labels as in our case for irrelevant/relevant recommendations.</li> </ol> <p>The Average Precision (AP) computes a scalar score for a single recommendation list according to the following definitions:</p> <p>Precision</p> \\[\\text{Precision} = \\frac{\\text{number of relevant items}}{\\text{number of items}}\\] <p>Average Precision (AP)</p> \\[\\text{AP} = \\frac{1}{r} \\sum_{k=1}^{K} P(k) \\cdot \\text{rel}(k)\\] <p>where:</p> <ul> <li>\\(K\\) is the total number of items,</li> <li>\\(r\\) is the total number of relevant items,</li> <li>\\(P(k)\\) is the precision at \\(k\\),</li> <li>\\(\\text{rel}(k)\\) is 1 if item \\(k\\) is relevant and 0 otherwise.</li> </ul> <p>If the labels are binary 0/1 encoded as in our case, the formula simplifies to:</p> \\[\\text{AP} = \\frac{1}{r} \\sum_{k=1}^{K} \\frac{\\sum_{i=1}^{k} \\text{rel}(i)}{k}\\] <p>The Mean Average Precision is then computed as the average over the Average Precision scores for the recommendations of all query documents in the training corpus.</p> <p>Mean Average Precision (MAP)</p> \\[\\text{MAP} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\text{AP}(q)\\] <p>where:</p> <ul> <li>\\(Q\\) is the total number of query documents,</li> <li>\\(\\text{AP}(q)\\) is the average precision for query document \\(q\\).</li> </ul> <p>Within this project, the MAP computes a scalar score for a given combination of Language Model Choice and Feature Weights. Thus, to determine which Recommender order works best within the Hybrid structure, we could e.g. aggregate the MAP scores for each order over all Language Model Choices and Feature Weights.</p> <p>Example</p> <p>The recommendation list [relevant, irrelvant, relevant] has a Precision of \\(P = \\frac{2}{3}\\) and an Average Precision of \\(AP = \\frac{1}{2} \\cdot (\\frac{1}{1} + \\frac{2}{3}) = \\frac{5}{6}\\).</p> <p>The recommendation list [relevant, relevant, irrelevant] has a Precision of \\(P = \\frac{2}{3}\\) and an Average Precision of \\(AP = \\frac{1}{2} \\cdot (\\frac{1}{1} + \\frac{2}{2}) = 1\\).</p> <p>The MAP of these two rankings is \\(MAP = \\frac{1}{2} \\cdot (\\frac{5}{6} + 1) = \\frac{11}{12}\\).</p>"},{"location":"reproducibility/","title":"Reproducibility","text":"<p>This section outlines the steps required to replicate the evaluation results featured in the thesis.</p>"},{"location":"reproducibility/#evaluation-results","title":"Evaluation Results","text":"<p>The evaluation results are described in detail in Chapter 4.2 of the thesis.</p> <p>The key findings are:</p> <ol> <li> <p>The Bibliographic Coupling feature is the most important feature for the Citation Recommender followed by the Co-Citation Analysis feature. The Paper Citation Count performs worst and is, on average, equally effective as randomly chosen papers from the training corpus.</p> </li> <li> <p>The SciBERT language model performs best for the Language Recommender followed by TF-IDF and BERT. The Longformer model cannot leverage its strength on long documents and performs worst.</p> </li> <li> <p>When using only a single recommender, the Language Recommender outperforms the Citation Recommender.</p> </li> <li> <p>The best hybrid model is the Language -&gt; Citation Hybrid Recommender, i.e. using the Language Recommender first for candidate selection and the Citation Recommender second for re-ranking.</p> </li> <li> <p>Surprisingly, the best overall model is not a hybrid model, but rather the Language Recommender with the SciBERT language model alone.</p> </li> </ol>"},{"location":"reproducibility/#evaluation-strategy","title":"Evaluation Strategy","text":"<p>To effectively replicate the evaluation results, understanding the evaluation strategy is crucial. This strategy is explained comprehensively in Chapter 4.1  of the thesis and summarized here. The following figure illustrates the strategy:</p> <p></p>"},{"location":"reproducibility/#step-1-finding-feature-weight-ranges","title":"Step 1: Finding Feature Weight Ranges","text":"<p>The first step aims to identify appropriate ranges for the feature weights of the Citation Recommender on the validation set. Its goal is to discover ranges that are neither too narrow nor too wide. Both extremes lead to suboptimal results: Overly narrow ranges lead to a highly restricted search space, which may exclude the optimal feature weights. Conversely, overly wide ranges may render the process computationally intractable. Further, the selected weights from a very broad search space may still be far from the optimal weights.</p> <p>The iterative search proceeds as follows:</p> <ul> <li>Initialize all feature weight ranges with the integer interval \\([0, 5]\\).</li> <li>Sample \\(n=50\\) random weight vectors and construct all combinations with the \\(1000\\) query documents in the validation set and the \\(8\\) language models, yielding an input space of \\(400,000\\) values.</li> <li>Randomly sample \\(N=1000\\) input combinations, ensuring an average of \\(20\\) samples for each weight vector.     Compute the marginal Mean Average Precision for all weight vectors, averaged over all query documents and language models, and analyze the \\(20\\) top-performing candidates.     If any weight within those vectors is at the boundaries of the current ranges, the ranges are expanded, as the optimal weights might lie outside the current ranges.     Subsequently, the sampling and evaluation steps are repeated with the adjusted ranges.     Conversely, if none of the weights are at the boundaries, the current ranges are deemed appropriate and the process moves to step 2.</li> </ul>"},{"location":"reproducibility/#step-2-randomized-feature-weights-search","title":"Step 2: Randomized Feature Weights Search","text":"<p>The second step, conducted on the validation set, assesses a larger number of feature weights within the ranges selected in step 1 and identifies the top \\(k=10\\) best-performing weight vectors.</p> <p>This step is performed as follows:</p> <ul> <li>Sample \\(n=200\\) feature weight combinations from the identified ranges of step 1, resulting in \\(1,600,000\\) input combinations, accounting for the \\(8\\) language models and \\(1000\\) query documents of the validation set.</li> <li>Randomly select \\(N=10,000\\) combinations, ensuring an average of \\(50\\) samples for each weight vector.</li> <li>Similar to step 1, compute the marginal Mean Average Precision for the weight vectors, averaged over all query documents and language models, and select the top \\(k=10\\) candidates for the final evaluation in step 3.</li> </ul>"},{"location":"reproducibility/#step-3-performance-evaluation","title":"Step 3: Performance Evaluation","text":"<p>The third step evaluates the inference performance of various input combinations on the test set.</p> <p>The evaluation procedure is as follows:</p> <ul> <li>Augment the selected feature weights of step 2 with the \\(6\\) baseline weight vectors \\([1, 0, 0, 0, 0], [0, 1, 0, 0, 0], \\ldots, [0, 0, 0, 0, 1], [1, 1, 1, 1, 1]\\) resulting in \\(k=16\\) candidates.     These simple weight combinations are added for interpretability: Each unit weight vector isolates the effect of a single feature while the vector of ones contributes the same impact to each of the five features.     Coupled with the \\(8\\) language models and \\(1000\\) query documents of the test set, the evaluation dataset comprises \\(128,000\\) combinations.</li> <li>Unlike previous steps, the evaluation is exhaustive without any prior subsampling. Inference runs for each of the \\(128,000\\) input combinations are initiated.     The resulting candidate and final recommendation rankings are evaluated using the Mean Average Precision.</li> </ul> <p>This final step is visualized in more detail in the following figure:</p> <p></p>"},{"location":"reproducibility/#implementation","title":"Implementation","text":""},{"location":"reproducibility/#feature-weight-search","title":"Feature Weight Search","text":"<p>The code for Steps 1 and 2 is contained in the script <code>s1_search_feature_weights.py</code>. Hyperparameters are configured at the beginning of the <code>main()</code> function:</p> <pre><code># For both steps\nseed = 42\n# Step 1\nnum_samples_feature_weights_candidates = 50\nnum_samples_input_combinations = 1_000\n# Step 2\nnum_samples_feature_weights_candidates = 200\nnum_samples_input_combinations = 10_000\n</code></pre> <p>The iterative search for weight ranges in step 1 requires the script to run multiple times sequentially, each time with the updated ranges from the previous run. The current weight ranges are specified as default values in the <code>FeatureWeightRanges</code> class.</p> <p>This process leads to (conservative) weight ranges of \\([0, 20]\\) for the publication date, paper citation count, and author citation count, and \\([0, 100]\\) for the co-citation analysis and bibliographic coupling features.</p> <p>The specific weight candidates found in step 2 are</p> <pre><code>[9, 12, 13, 68, 95],\n[0, 3, 17, 96, 34],\n[18, 10, 6, 83, 63],\n[2, 7, 12, 15, 72],\n[2, 13, 18, 72, 66],\n[9, 9, 14, 9, 8, 87],\n[16, 15, 5, 84, 10],\n[10, 13, 19, 65, 14],\n[17, 2, 18, 54, 4],\n[9, 19, 20, 67, 1]\n</code></pre> <p>where the positions correspond to the order [publication date, paper citation count, author citation count, co-citation analysis, bibliographic coupling].</p>"},{"location":"reproducibility/#performance-evaluation","title":"Performance Evaluation","text":"<p>The script <code>s2_compute_metrics.py</code> carries out the performance evaluation in Step 3. This script requires no additional configuration. As the evaluation is exhaustive and no subsampling is performed, no random seed is required for reproducibility. On a standard local machine, the \\(128.000\\) inference runs take approximately \\(50\\) hours to complete.</p> <p>To reproduce all plots and tables in the thesis, refer to the plots directory in the msc-thesis repository. To use the evaluation results generated by step 3 above, set the <code>RESULTS_DIRPATH</code> environment variable to point to the same local directory in both projects.</p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#data-and-models","title":"Data and Models","text":"<p>Note</p> <p>To execute all scripts and reproduce project results, some local downloads are necessary as prerequisites, including data files and pretrained models.</p>"},{"location":"setup/#data","title":"Data","text":"<p>There are three data sources for this project:</p> <ol> <li> <p>D3 Dataset</p> <p>The D3 DBLP Discovery Dataset is a compilation of metadata for over 6 million computer science papers. It is the primary data source for this project. All three global document features as well as the paper abstracts are provided by this dataset.</p> <p>The dataset consists of two files with information about documents and authors, respectively. They can be downloaded from the Zenodo repository.</p> </li> <li> <p>Arxiv Labels</p> <p>Arxiv categories are used as labels for evaluating the Hybrid Recommender's performance. A binary classification task is defined: A candidate paper is considered a relevant recommendation if it shares at least one arXiV label with the query paper, and irrelevant otherwise. Arxiv labels are extracted from the arxiv-metadata-oai-snapshot.json dataset on Kaggle.</p> </li> <li> <p>Citation Information</p> <p>To obtain individual citations and references to compute co-citation analysis and bibliographic coupling scores, the Semantic Scholar API is fetched. A private API key is recommended for a higher request rate.</p> </li> </ol>"},{"location":"setup/#models","title":"Models","text":"<p>The following pretrained Word2Vec, GloVe, and FastText models are used as static embedding models:</p> <ul> <li>Pretrained word2vec-google-news-300 Word2Vec model from Gensim</li> <li>Pretrained glove.6B GloVe model from the Stanford NLP website</li> <li>Pretrained English FastText model from the FastText website</li> </ul>"},{"location":"setup/#environment-variables","title":"Environment Variables","text":"<p><code>readnext</code> needs to know the locations of local data and model files in your file system, which can be stored in any directory. User-specific information is provided through environment variables. The <code>.env_template</code> file in the project root directory contains a template for the expected environment variables with default values (except for the Semantic Scholar API key):</p> .env_template<pre><code>DOCUMENTS_METADATA_FILENAME=\"2022-11-30_papers.jsonl\"\nAUTHORS_METADATA_FILENAME=\"2022-11-30_authors.jsonl\"\nARXIV_METADATA_FILENAME=\"arxiv_metadata.json\"\nSEMANTICSCHOLAR_API_KEY=\"ABC123\"\nDATA_DIRPATH=\"data\"\nMODELS_DIRPATH=\"models\"\nRESULTS_DIRPATH=\"results\"\n</code></pre> <p>Explanation of the environment variables:</p> <ul> <li><code>DOCUMENTS_METADATA_FILENAME</code> and <code>AUTHORS_METADATA_FILENAME</code> correspond to the downloaded D3 dataset files, <code>ARXIV_METADATA_FILENAME</code> to the downloaded arxiv dataset file.</li> <li><code>SEMANTICSCHOLAR_API_KEY</code> represents the API key for the Semantic Scholar API.</li> <li><code>DATA_DIRPATH</code> is the directory path for all local data files, including downloaded and generated data files.</li> <li><code>MODELS_DIRPATH</code> is the directory path for all pretrained model files.</li> <li><code>RESULTS_DIRPATH</code> is the directory path for all stored result files, such as tokenized abstracts, numeric embeddings of abstracts, and precomputed co-citation analysis, bibliographic coupling, and cosine similarity scores.</li> </ul>"},{"location":"setup/#setup-scripts","title":"Setup Scripts","text":"<p>The inference step of the <code>readnext</code> package leverages preprocessed and precomputed data such that all recommender features and abstract embeddings are readily available. To generate these files locally, run the following setup scripts in the specified order. All scripts are located in the <code>readnext/scripts</code> directory.</p> <ol> <li> <p>Dataset Construction</p> <p>These scripts are located in the <code>readnext/scripts/data</code> directory.</p> <ol> <li><code>s1_read_raw_data.py</code>: Reads documents, authors and arxiv metadata from raw JSON files and write it out into Parquet format.</li> <li><code>s2_merge_arxiv_labels.py</code>: Merges the arxiv metadata with the D3 dataset via the arxiv id. Adds arxiv labels as new feature to the dataset which are later used as ground-truth labels for the recommender system.</li> <li><code>s3_merge_authors.py</code>: Adds the author citationcount to the dataset and selects the most popular author for each document.</li> <li><code>s4_add_citations.py</code>: Sends requests to the semanticscholar API to obtain citation and reference urls for all documents in the dataset and add them as features to the dataframe.</li> <li><code>s5_add_ranks.py</code>: Adds rank features for global document characteristics (publication date, document citation count and author citation count) to the dataset and selects a subset of the most cited documents for the final dataset.</li> </ol> </li> </ol> <p>All further script paths are relative to the <code>readnext/scripts/modeling</code> directory.</p> <ol> <li> <p>Citation Models</p> <ol> <li><code>run_co_citation_analysis.py</code>: Precomputes co-citation analysis scores for all document pairs in the dataset.</li> <li><code>bibliographic_coupling.py</code>: Precomputes bibliographic coupling scores for all document pairs in the dataset.</li> </ol> </li> <li> <p>Language Models</p> <ol> <li><code>tokenizer/run_tokenizer.py</code>: Tokenizes the abstracts of all documents in the dataset by four different tokenizers into the appropriate format for all eight language models.</li> <li><code>embedder/run_embedder_*.py</code>: These scripts generate sparse or dense numeric embeddings of all document abstracts for each language model. The process is split into separate scripts for each model to allow for easy parallelization.</li> <li><code>cosine_similarities/run_cosine_similarities_*.py</code>: Precomputes cosine similarity scores for all document pairs in the dataset for each language model. Again, multiple scripts are used for parallelization purposes.</li> </ol> <p>Note</p> <p>The <code>run_embedder_*.py</code> and <code>run_cosine_similarities_*.py</code> scripts are independent between different language models! That means that you can run a subset of scripts only for those language models that you want to use for the recommender system. For example, if you are only interested in the Longformer language model, it is sufficient to run the scripts <code>run_embedder_longformer.py</code> and <code>run_cosine_similarities_longformer.py</code> in steps b and c, respectively.</p> </li> </ol>"},{"location":"setup/#development-workflow","title":"Development Workflow","text":"<p>This project utilizes pdm for package and dependency management. To install <code>pdm</code>, follow the installation instructions on the pdm website.</p> <p>Pdm provides the option to define user scripts that can be run from the command line. These scripts are specified in the <code>pyproject.toml</code> file in the <code>[tool.pdm.scripts]</code> section.</p> <p>The following built-in and custom user scripts are useful for the development workflow:</p> <ul> <li><code>pdm add &lt;package name&gt;</code>: Add and install a new (production) dependency to the project.     They are automatically added to the <code>[project]</code> -&gt; <code>dependencies</code> section of the <code>pyproject.toml</code> file.</li> <li><code>pdm add -dG dev &lt;package name&gt;</code>: Add and install a new development dependency to the project.     They are automatically added to the <code>[tool.pdm.dev-dependencies]</code> section of the <code>pyproject.toml</code> file.</li> <li><code>pdm remove &lt;package name&gt;</code>: Remove and uninstall a dependency from the project.</li> <li><code>pdm format</code>: Format the entire project with black.     The black configuration is specified in the <code>[tool.black]</code> section of the <code>pyproject.toml</code> file.</li> <li><code>pdm lint</code>: Lint the entire project with the ruff linter.     The ruff configuration is specified in the <code>[tool.ruff.*]</code> sections of the <code>pyproject.toml</code> file.</li> <li><code>pdm check</code>: Static type checking with mypy.     The mypy configuration is specified in the <code>[tool.mypy]</code> section of the <code>pyproject.toml</code> file.</li> <li><code>pdm test*</code>: Collection of commands to run unit tests with pytest.     The pytest configuration is specified in the <code>[tool.pytest.ini_options]</code> section of the <code>pyproject.toml</code> file.     The individual commands are:<ul> <li><code>pdm test</code>: Run all unit tests.</li> <li><code>pdm test-cov</code>: Run all unit tests and generate a test coverage report.</li> <li><code>pdm test-fast</code>: Run only tests without extensive computations. Useful for a quick feedback loop during development.</li> <li><code>pdm test-slow</code>: Opposite of the <code>pdm test-fast</code> command. Runs only tests that require significant computation time.</li> <li><code>pdm test-ci</code>: Run only tests that are executed in the continuous integration pipeline. This excludes all tests that depend on precomputed local data files not available in the CI environment.</li> </ul> </li> <li><code>pdm pre</code>: Run pre-commit on all files.     All pre-commit hooks are specified in the <code>.pre-commit-config.yaml</code> file in the project root directory.</li> <li><code>pdm serve</code>: Preview the project documentation locally.</li> <li><code>pdm deploy</code>: Deploy the project documentation to GitHub pages.</li> </ul>"}]}